# Parameters for the git-sync deployment. git-sync pulls code from a repo and saves it in the PVC defined in the dags-pvc.yaml.
# That PVC stores data in Azure File Share and can be then mounted into other pods to make the code available in them. 
#   - repoURL - URL of the repo with dags code to pull (https://github.com/<org-name>/<repo-name>.git)
#   - repoName - Name of the repo (<repo-name>.git)
#   - branch - Branch with code to use
#   - clonePath - Where to clone code in the git-sync pod (which will be mounted into the PVC)
#   - storageAccountName - Name of the Azure Storage Account where to save dags code
#   - fileShareName - Name of the File Share within the Azure Storage Account where to save dags code
#   - secretName - Name of the secret for accessing Storage Account with File Share. It must contains keys:
#       - azurestorageaccountname - Storage Account name
#       - azurestorageaccountkey - Access key
gitSync:
  repoURL: https://github.com/bulka4/data_and_ml_platform.git
  repoName: ${repo_name}
  dagsFolderPath: ${dags_folder_path}
  branch: main
  clonePath: /opt/airflow/dags
  storageAccountName: ${airflow_logs_sa_name}
  fileShareName: ${airflow_dags_fs_name}
  secretName: ${storage_account_secret}

# Secret for pulling images from ACR
acr:
  secretName: ${acr_secret_name}

airflow:
  executor: CeleryExecutor

  images:
    airflow:
      repository: ${acr_url}/${airflow_image_name}
      tag: ${airflow_image_tag}
  # Secret for pulling images from ACR
  registry:
    secretName: ${acr_secret_name}
  env:
    - name: AIRFLOW__CORE__LOAD_EXAMPLES
      value: "false"
    - name: AIRFLOW__KUBERNETES_EXECUTOR__NAMESPACE
      value: ${airflow_kubernetes_namespace}
    # Saving logs to Azure Storage Account
    - name: AIRFLOW__LOGGING__REMOTE_LOGGING
      value: "True"
    - name: AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER
      value: "wasb://${airflow_logs_container_name}@${airflow_logs_sa_name}.blob.core.windows.net"
    - name: AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID
      value: "azure_blob" # ID of the connection used for accessing Azure Storage Account for saving Airflow logs
    # Use this service accounts for all pods running Airflow tasks
    - name: AIRFLOW__KUBERNETES__SERVICE_ACCOUNT_NAME
      value: airflow-sa
    # We need to provide webserver URL to see logs in UI. Looks like this URL is sent to a browser and browser uses it
    # to get data about logs.
    # - name: AIRFLOW__WEBSERVER__BASE_URL
    #   value: "http://airflow-webserver"

  # Add PVC with code pulled by the git-sync to the scheduler, workers, triggerer and webserver
  scheduler:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa

  workers:
    # Limit the size of storage for logs
    persistence:
      size: 1Gi
    extraVolumes:
      # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV.
      # It will be used by the copier container
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc
      # Ephemeral volume to share dags with the worker container
      - name: tmp-dags
        emptyDir: {}
    # extraVolumeMounts:
    #   - name: dags
    #     mountPath: /opt/airflow/dags
    extraVolumeMounts:
      - name: tmp-dags
        mountPath: /opt/airflow/dags
    # Container into which PVC with dags is mounted and copies dags into an ephemeral volume which is then accessed by
    # the worker container
    extraContainers:
      - name: copier
        image: alpine:3.19
        command:
          - sh
          - -c
          - |
            # Install rsync
            apk add --no-cache rsync

            while true; do
              # Copy file differences from a PV into an ephemeral volume
              # Remove files at target which are not present at the source anymore
              rsync -a --delete /mnt/remote_dags/ /tmp/dags/
              sleep 60
            done
        volumeMounts:
          - name: tmp-dags
            mountPath: /tmp/dags
          - name: dags
            mountPath: /mnt/remote_dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa


  triggerer:
    # Limit the size of storage for logs
    persistence:
      size: 1Gi
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa

  # Set to false in order not to create a PostgreSQL for Airflow metadata (we create it separately in postgresql.yaml)
  postgresql:
    enabled: false

  # Use the below configuration if we use an external PostgreSQL db, not the one created in this chart by setting
  # postgresql.enabled = true in this file.
  data:
    # Use the secret with the 'connection' key with the connection string to Postgres which will be used by Airflow to connect.
    metadataSecretName: airflow-postgres-connection

  webserver:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        #subPath: dags
    # Use our own service account which has specified a secret for pulling images.
    serviceAccount:
      create: False
      name: airflow-sa
    # Use the LoadBalancer as a service for the web server, so we can access it through the internet from outside of the Kubernetes cluster
    service:
      type: LoadBalancer
    # Add this setting to avoid the 'Startup probe failed' error in the api-server pod. We still get that warning even with those settings
    # but everything works now so I will keep those settings just in case (maybe they can be removed later).
    startupProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    livenessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    readinessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Increase resource limits and requests so processes in the api-server pod don't die (we can check in the api-server pod logs if they die)
    resources:
      # Requests will be automatically set up to the same value as the limits
      limits:
        cpu: "2000m"
        memory: "2Gi"
    # Run additional init containers to execute commands for additional setup:
    #   - Create a user for accessing UI
    #   - Create a connection to Azure Storage Account for saving logs there
    extraInitContainers:
      # Createe an Airflow user for accessing UI. For some reason Helm parameters for creating the default user doesn't work
      - name: create-admin-user
        image: ${acr_url}/${airflow_image_name}:${airflow_image_tag}
        imagePullPolicy: IfNotPresent
        # Create a new user. Use '|| true' (|| is the logical operator OR) so that command doesn't fail when user already exists.
        command:
          - bash
          - -c
          - |
            airflow users create \
              --username admin \
              --firstname Admin \
              --lastname User \
              --role Admin \
              --email admin@example.com \
              --password admin || true
        env:
          # Add env var with a connection string to the PostgreSQL metadata db we use, so Airflow knows which one to use and can connect.
          # Data about the new user will be saved there.
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                name: airflow-postgres-connection
                key: connection
      # Create a connection for accessing an Azure Storage Account where Airflow will be saving logs. It uses client ID and password of a Service Principal
      # with permissions to that Storage Account. Values are taken from a Kubernetes secret which needs to be created before deploying this chart.
      - name: create-azure-connection
        image: ${acr_url}/${airflow_image_name}:${airflow_image_tag}
        imagePullPolicy: IfNotPresent
        # Create a new connection if it doesn't exist yet (|| is the logical operator OR)
        command:
          - bash
          - -c
          - |
            airflow connections get azure_blob || \
            airflow connections add azure_blob \
              --conn-type wasb \
              --conn-login $ACCOUNT_NAME \
              --conn-password $ACCESS_KEY
        env:
          - name: ACCOUNT_NAME
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: azurestorageaccountname
          - name: ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: azurestorageaccountkey
          # Add env var with a connection string to the PostgreSQL metadata db we use, so Airflow knows which one to use and can connect.
          # Data about the new connection will be saved there.
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                name: airflow-postgres-connection
                key: connection