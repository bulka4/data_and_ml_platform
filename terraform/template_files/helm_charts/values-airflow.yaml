executor: KubernetesExecutor

# Parameters for the git-sync deployment. git-sync pulls code from a repo and saves it in the PVC defined in the dags-pvc.yaml.
# That PVC can be then mounted into other pods to make the code available in them. 
gitSync:
  repoURL: ${repo_url}  # https://github.com/<org-name>/<repo-name>.git
  branch: ${branch}     # Branch with code to use
  clonePath: /opt/airflow/dags # Where to clone code in the git-sync pod (which will be mounted into the PVC)

airflow:
  images:
    airflow:
      repository: ${acr_url}/${airflow_image_name}
      tag: ${airflow_image_tag}
  # Secret for pulling images from ACR
  registry:
    secretName: ${acr_secret_name}
  config:
    AIRFLOW__CORE__LOAD_EXAMPLES:
      value: "false"
    AIRFLOW__KUBERNETES__NAMESPACE:
      value: ${airflow_kubernetes_namespace}
    # Saving logs to Azure Storage Account
    AIRFLOW__LOGGING__REMOTE_LOGGING:
      value: "True"
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER:
      value: "wasb://${airflow_logs_container_name}@${airflow_logs_sa_name}.blob.core.windows.net"
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID:
      value: "azure_blob" # ID of the connection used for accessing Azure Storage Account for saving Airflow logs

  # Add PVC with code pulled by the git-sync to the scheduler, workers, triggerer and webserver
  scheduler:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        subPath: ${repo_dags_folder_path} # Mount only a specific path from the volume

  workers:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        subPath: ${repo_dags_folder_path} # Mount only a specific path from the volume

  triggerer:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        subPath: ${repo_dags_folder_path} # Mount only a specific path from the volume

  webserver:
    extraVolumes:
      - name: dags
        persistentVolumeClaim:
          claimName: airflow-dags-pvc # Volume with dags pulled by git-sync (running in a separate pod) and saved in that PV
    extraVolumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags
        subPath: ${repo_dags_folder_path} # Mount only a specific path from the volume

  # Set to false in order not to create a PostgreSQL for Airflow metadata (we create it separately in postgresql.yaml)
  postgresql:
    enabled: false

  # Use the below configuration if we use an external PostgreSQL db, not the one created in this chart by setting
  # postgresql.enabled = true in this file.
  data:
    # Use the secret with the 'connection' key with the connection string to Postgres which will be used by Airflow to connect.
    metadataSecretName: airflow-postgres-connection

  # Limit the size of storage for logs
  workers:
    persistence:
      size: 1Gi

  triggerer:
    persistence:
      size: 1Gi

  webserver:
    # Use the LoadBalancer as a service for the web server, so we can access it through the internet from outside of the Kubernetes cluster
    service:
      type: LoadBalancer
    # Add this setting to avoid the 'Startup probe failed' error in the api-server pod. We still get that warning even with those settings
    # but everything works now so I will keep those settings just in case (maybe they can be removed later).
    startupProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    livenessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Similar as for the startupProbe
    readinessProbe:
      timeoutSeconds: 60
      periodSeconds: 10
      failureThreshold: 20
    # Increase resource limits and requests so processes in the api-server pod don't die (we can check in the api-server pod logs if they die)
    resources:
      # Requests will be automatically set up to the same value as the limits
      limits:
        cpu: "2000m"
        memory: "2Gi"
    # Run additional init containers to execute commands for additional setup:
    #   - Create a user for accessing UI
    #   - Create a connection to Azure Storage Account for saving logs there
    extraInitContainers:
      # Createe an Airflow user for accessing UI. For some reason Helm parameters for creating the default user doesn't work
      - name: create-admin-user
        image: ${acr_url}/${airflow_image_name}:${airflow_image_tag}
        imagePullPolicy: IfNotPresent
        command:
          - bash
          - -c
          - |
            airflow users create \
              --username admin \
              --firstname Admin \
              --lastname User \
              --role Admin \
              --email admin@example.com \
              --password admin || true
        env:
          # Add env var with a connection string to the PostgreSQL metadata db we use, so Airflow knows which one to use and can connect
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                name: airflow-postgres-connection
                key: connection
      # Create a connection for accessing an Azure Storage Account where Airflow will be saving logs. It uses client ID and password of a Service Principal
      # with permissions to that Storage Account. Values are taken from a Kubernetes secret which needs to be created before deploying this chart.
      - name: create-azure-connection
        image: ${acr_url}/${airflow_image_name}:${airflow_image_tag}
        imagePullPolicy: IfNotPresent
        command:
          - bash
          - -c
          - |
            airflow connections add azure_blob \
              --conn-type wasb \
              --conn-login $AZURE_CLIENT_ID \
              --conn-password $AZURE_CLIENT_SECRET \
              --conn-extra '{"tenant_id":"$AZURE_TENANT_ID","account_name":"$AZURE_ACCOUNT_NAME"}'
        env:
          - name: AZURE_TENANT_ID
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: tenant_id
          - name: AZURE_ACCOUNT_NAME
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: account_name
          - name: AZURE_CLIENT_ID
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: client_id
          - name: AZURE_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: ${storage_account_secret}
                key: client_secret
          # Add env var with a connection string to the PostgreSQL metadata db we use, so Airflow knows which one to use and can connect
          - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
            valueFrom:
              secretKeyRef:
                name: airflow-postgres-connection
                key: connection